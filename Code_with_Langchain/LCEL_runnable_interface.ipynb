{"cells":[{"cell_type":"markdown","metadata":{"id":"N6uzLHs7CCt6"},"source":["# LangChain Expression Language"]},{"cell_type":"markdown","source":["LangChain Expression Language (LCEL) is a feature within the LangChain framework that allows users to compose complex workflows for language models, tools, and data transformations using a declarative, functional programming style. It simplifies how you build workflows or pipelines by using **expressions** and **runnables** to handle operations on inputs, intermediate steps, and outputs.\n","\n","LangChain Expression Language helps in creating modular and reusable code for chaining together tasks like:\n","\n","- Interacting with APIs\n","- Calling language models\n","- Running data transformations\n","- Managing I/O between different steps\n","\n","### Key Concepts of LangChain Expression Language (LCEL)\n","\n","1. **Runnables**:\n","   - A core part of LCEL, **runnables** represent steps in a workflow (like an API call, data transformation, or a model call).\n","   - You can chain multiple runnables to build more complex pipelines. For example, you can first preprocess input text, then call a language model, and finally post-process the model's output.\n","\n","2. **Expressions**:\n","   - **Expressions** are reusable, functional operations in LangChain that can be applied to inputs, manipulated, and returned in various forms.\n","   - These expressions are essentially lightweight functions or operations that can be combined using operators like `|` (pipe) to form more complex logic.\n","\n","3. **Pipe (`|`) Operator**:\n","   - The `|` (pipe) operator allows you to **compose sequences of runnables** or functions. This operator passes the result of one function as the input to the next function, simplifying complex workflows.\n","   - For example, `step1 | step2 | step3` passes the output of `step1` to `step2`, then the output of `step2` to `step3`.\n","\n","4. **Lambda Functions**:\n","   - LCEL allows you to wrap Python **lambda functions** or standard functions as **runnable objects** that can participate in workflows. This makes it possible to easily integrate custom logic.\n","\n","5. **Flow Control and Data Transformation**:\n","   - You can define branching logic, parallel executions, or transformations on the data within a chain of runnables.\n","   - For example, you can split text, filter it, run different processing pipelines on different parts of the data, and then aggregate the results back.\n","\n","6. **`invoke` and `batch` Methods**:\n","   - The `invoke` method is used to process a single input through the entire pipeline of runnables.\n","   - The `batch` method is used to process multiple inputs in parallel through the same pipeline.\n","\n","### Example\n","\n","Here’s a simple example of how LangChain Expression Language can be used to chain operations:\n","\n","```python\n","from langchain.schema.runnable import RunnableLambda\n","\n","# Create individual runnable steps\n","step1 = RunnableLambda(lambda x: x + 1)\n","step2 = RunnableLambda(lambda x: x * 2)\n","step3 = RunnableLambda(lambda x: x ** 2)\n","\n","# Combine them using the pipe operator\n","sequence = step1 | step2 | step3\n","\n","# Process a single input\n","print(sequence.invoke(1))  # Output: 16  ( (1 + 1) * 2 ) ^ 2 = 16\n","\n","# Process a batch of inputs\n","print(sequence.batch([1, 2, 3]))  # Output: [16, 36, 64]\n","```\n","\n","### Advanced Use Case Example: Calling Language Models and APIs\n","\n","```python\n","from langchain.schema.runnable import RunnableLambda\n","from langchain.llms import OpenAI\n","\n","# Load a pre-trained language model from OpenAI\n","llm = OpenAI(model=\"gpt-3.5-turbo\")\n","\n","# Define the workflow\n","sequence = (\n","    RunnableLambda(lambda x: f\"Question: {x}\")  # Add 'Question:' prefix\n","    | llm.run  # Pass the question to the language model\n","    | RunnableLambda(lambda x: x.strip())  # Post-process the model's response\n",")\n","\n","# Use the sequence to generate answers\n","output = sequence.invoke(\"What is the capital of France?\")\n","print(output)  # Likely output: \"Paris\"\n","```\n","\n","In this example:\n","1. A string is transformed into a formatted question.\n","2. The formatted question is passed to the GPT-3.5 language model using OpenAI's API.\n","3. The response from the model is post-processed to remove leading/trailing spaces.\n","\n","### Summary of Key Operations\n","\n","- **RunnableLambda**: Wraps a lambda function or any callable into a runnable step.\n","- **Pipe (`|`)**: Chains multiple operations (steps) together to form a processing sequence.\n","- **invoke()**: Runs the entire pipeline for a single input.\n","- **batch()**: Runs the entire pipeline for a list of inputs in parallel.\n","\n","### Benefits of LangChain Expression Language\n","- **Declarative Programming**: Focus on **what** to do rather than **how** to do it. LCEL allows you to describe the flow of your logic declaratively.\n","- **Modular and Reusable**: You can easily reuse components in different workflows, making your code cleaner and more maintainable.\n","- **Composability**: Complex logic is broken into smaller, manageable chunks that can be composed together seamlessly.\n","\n","LCEL helps you build powerful data pipelines and language model workflows with simplicity and composability in mind."],"metadata":{"id":"huSFIftrqx1_"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"ykL6lGhHCCt7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727710192140,"user_tz":-180,"elapsed":10051,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"c4de49fa-7328-4012-dd2e-f3a4f2770c82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain_openai\n","  Downloading langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting langchain-core<0.4.0,>=0.3.6 (from langchain)\n","  Downloading langchain_core-0.3.6-py3-none-any.whl.metadata (6.3 kB)\n","Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n","  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n","  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n","Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n","  Downloading openai-1.50.2-py3-none-any.whl.metadata (24 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain_openai)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.6->langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.12.2)\n","Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n","Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n","  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain)\n","  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n","Downloading langchain-0.3.1-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_openai-0.2.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.6-py3-none-any.whl (399 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n","Downloading langsmith-0.1.129-py3-none-any.whl (292 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.50.2-py3-none-any.whl (382 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.0/383.0 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n","Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, jiter, h11, tiktoken, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langchain-text-splitters, langchain_openai, langchain\n","  Attempting uninstall: tenacity\n","    Found existing installation: tenacity 9.0.0\n","    Uninstalling tenacity-9.0.0:\n","      Successfully uninstalled tenacity-9.0.0\n","Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.1 langchain-core-0.3.6 langchain-text-splitters-0.3.0 langchain_openai-0.2.1 langsmith-0.1.129 openai-1.50.2 orjson-3.10.7 tenacity-8.5.0 tiktoken-0.7.0\n"]}],"source":["%pip install langchain langchain_openai --upgrade"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"qa7dtwiXCCt8","executionInfo":{"status":"ok","timestamp":1727710270214,"user_tz":-180,"elapsed":248,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["# import os\n","# os.environ[\"OPENAI_API_KEY\"] = \"API_KEY_HERE\""]},{"cell_type":"code","source":["import getpass\n","import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rRJPiHwTapdf","executionInfo":{"status":"ok","timestamp":1727710266281,"user_tz":-180,"elapsed":11828,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"6ec36229-d13b-4283-9cad-fcdc2a439e01"},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}]},{"cell_type":"markdown","metadata":{"id":"dvyORyO3CCt8"},"source":["Explain the basic syntax of [LangChain Expression Language](https://python.langchain.com/docs/expression_language/), which uses the pipe symbol `|` to connect components. Each component represents a specific task or action."]},{"cell_type":"markdown","metadata":{"id":"18CoGtObCCt8"},"source":["To make it as easy as possible to create custom chains, LangChain implemented a `Runnable` protocol. The Runnable protocol is implemented for most components. This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way. The standard interface includes:\n","\n","- `stream`: stream back chunks of the response\n","- `invoke`: call the chain on an input\n","- `batch`: call the chain on a list of inputs\n","\n","These also have corresponding async methods:\n","\n","- `astream`: stream back chunks of the response async\n","- `ainvoke`: call the chain on an input async\n","- `abatch`: call the chain on a list of inputs async\n","- `astream_log`: stream back intermediate steps as they happen, in addition to the final response\n","\n","----"]},{"cell_type":"markdown","metadata":{"id":"FpNFWXQACCt8"},"source":["## The Runnable Protocol:\n","\n","A unit of work that can be invoked, batched, streamed, transformed and composed.\n","\n","All methods accept an optional config argument, which can be used to configure execution, add tags and metadata for tracing and debugging etc.\n","\n","Runnables expose schematic information about their input, output and config via the input_schema property, the output_schema property and config_schema method.\n","\n","The LangChain Expression Language (LCEL) is a declarative way to compose Runnables into chains. Any chain constructed this way will automatically have sync, async, batch, and streaming support.\n","\n","The main composition primitives are `RunnableSequence` and `RunnableParallel`.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ARihzNo5CCt8"},"source":["## RunnableLambda"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"5fLtAEqHCCt8","outputId":"2dbdcc4e-362c-46dd-daff-30ee9c719f58","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727710829981,"user_tz":-180,"elapsed":757,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'langchain_core.runnables.base.RunnableLambda'>\n"]}],"source":["from langchain_core.runnables import RunnableLambda\n","\n","print(type(RunnableLambda(lambda x: x + 1))) # <class 'langchain.schema.runnable.RunnableLambda'>"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Uw0MfnzqCCt9","executionInfo":{"status":"ok","timestamp":1727710851372,"user_tz":-180,"elapsed":327,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["chain = RunnableLambda(lambda x: x + 1)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jEXiiXIMCCt9","outputId":"44bfd7fb-248d-4247-d8e8-432a548f5e39","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727710870835,"user_tz":-180,"elapsed":539,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":6}],"source":["chain.invoke(1)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"y1I_BpwrCCt9","outputId":"10baaf4b-be26-41f5-80ae-efbcca96f05c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727710892847,"user_tz":-180,"elapsed":242,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":7}],"source":["chain.invoke(5)"]},{"cell_type":"markdown","metadata":{"id":"AeyTfcQfCCt9"},"source":["## RunnableSequence\n","\n","`RunnableSequence` invokes a series of runnables sequentially, with one runnable’s output serving as the next’s input. Construct using the `|` operator or by passing a list of runnables to RunnableSequence."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"xnuQG8j8CCt9","outputId":"bf23286b-1521-4884-8a09-0338a8a72b44","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727710920941,"user_tz":-180,"elapsed":258,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'langchain_core.runnables.base.RunnableSequence'>\n","\n","\n","---\n","4\n"]},{"output_type":"execute_result","data":{"text/plain":["[4, 6, 8]"]},"metadata":{},"execution_count":8}],"source":["# A RunnableSequence constructed using the `|` operator\n","sequence = RunnableLambda(lambda x: x + 1) | (lambda x: x * 2)\n","\n","print(type(sequence)) # <class 'langchain.schema.runnable.RunnableSequence'>\n","print('\\n\\n---')\n","print(sequence.invoke(1)) # 4\n","sequence.batch([1, 2, 3]) # [4, 6, 8]"]},{"cell_type":"markdown","source":["In this example, we're working with a **RunnableSequence** object created by chaining two operations using the `|` (pipe) operator. The `RunnableSequence` is part of the **LangChain** framework (or a similar one) and is designed to process a series of actions in sequence. Here's a detailed breakdown of how this works:\n","\n","### Code Explanation\n","\n","1. **RunnableLambda**:\n","   - `RunnableLambda` is a class that takes a lambda function (or any callable) and wraps it as a runnable object.\n","   - In this case, the first `RunnableLambda` wraps the lambda function `lambda x: x + 1`, which adds 1 to its input.\n","\n","   ```python\n","   RunnableLambda(lambda x: x + 1)\n","   ```\n","\n","2. **Chaining with `|` (Pipe Operator)**:\n","   - In LangChain (and possibly other frameworks), the pipe (`|`) operator can be used to chain two runnable objects together.\n","   - Here, you are chaining `RunnableLambda(lambda x: x + 1)` with a plain lambda function `lambda x: x * 2`.\n","   - When two functions are chained using `|`, the output of the first function is passed as input to the second function.\n","   \n","   ```python\n","   sequence = RunnableLambda(lambda x: x + 1) | (lambda x: x * 2)\n","   ```\n","\n","   This `sequence` object is now an instance of `RunnableSequence`, which chains these two operations together.\n","\n","3. **Type of `sequence`**:\n","   - The `sequence` object is an instance of `RunnableSequence`, which is a class that holds a sequence of operations.\n","   \n","   ```python\n","   print(type(sequence))  # Output: <class 'langchain.schema.runnable.RunnableSequence'>\n","   ```\n","\n","4. **Executing the Sequence (Using `invoke` method)**:\n","   - When you call `sequence.invoke(1)`, the following happens:\n","     - First, the lambda function `lambda x: x + 1` is invoked with `x = 1`, producing `1 + 1 = 2`.\n","     - Then, the result (`2`) is passed to the second lambda function `lambda x: x * 2`, producing `2 * 2 = 4`.\n","   \n","   Therefore, the output of `sequence.invoke(1)` is `4`.\n","\n","   ```python\n","   sequence.invoke(1)  # Output: 4\n","   ```\n","\n","5. **Batch Processing (Using `batch` method)**:\n","   - The `batch` method processes a list of inputs by running each input through the entire sequence of operations.\n","   - Here, `sequence.batch([1, 2, 3])` works as follows:\n","     - For `1`: The sequence computes `(1 + 1) * 2 = 4`.\n","     - For `2`: The sequence computes `(2 + 1) * 2 = 6`.\n","     - For `3`: The sequence computes `(3 + 1) * 2 = 8`.\n","   \n","   So, the output is `[4, 6, 8]`.\n","\n","   ```python\n","   sequence.batch([1, 2, 3])  # Output: [4, 6, 8]\n","   ```\n","\n","### Key Concepts\n","\n","- **RunnableLambda**: This class wraps a lambda function to make it runnable and compatible with LangChain's runnable interfaces.\n","- **RunnableSequence**: This class represents a chain of runnable functions. You can create one by chaining functions with the `|` operator.\n","- **`invoke` Method**: This method processes a single input through the entire chain of operations.\n","- **`batch` Method**: This method processes a list of inputs in parallel through the chain of operations.\n","\n","### Summary\n","\n","The code sets up a **RunnableSequence** where:\n","- The first operation adds 1 to the input.\n","- The second operation multiplies the result by 2.\n","\n","By chaining operations with the `|` operator, you create a sequence of transformations, which can then be executed on single or multiple inputs."],"metadata":{"id":"WoWxQ5FFna7_"}},{"cell_type":"markdown","metadata":{"id":"16L2_FOuCCt9"},"source":["## RunnableParallel\n","\n","The `RunnableParallel`, allows for multiple runnables to be invoked in parallel, construct using a dictionary of runnables to invoke in parallel."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"7yXGOl26CCt-","outputId":"4bd87cf6-6a95-429d-d5ac-14c84b3b9ad6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727711311377,"user_tz":-180,"elapsed":226,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'mul_2': 4, 'mul_5': 10}"]},"metadata":{},"execution_count":9}],"source":["# A sequence that contains a RunnableParallel constructed using a dict literal\n","sequence = RunnableLambda(lambda x: x + 1) | {\n","    \"mul_2\": RunnableLambda(lambda x: x * 2),\n","    \"mul_5\": RunnableLambda(lambda x: x * 5),\n","}\n","sequence.invoke(1)  # {'mul_2': 4, 'mul_5': 10}"]},{"cell_type":"markdown","metadata":{"id":"iCj9qh97CCt-"},"source":["---------------\n","\n","## Combining the output of multiple runnables into a single response\n","\n","A sequence that contains a RunnableParallel constructed using a dict literal, this is then followed by a RunnableLambda that consumes the output of the RunnableParallel"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"1aJuZ0KJCCt-","outputId":"22bd77e9-738b-4d00-a40f-192bb285c2ba","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727711709885,"user_tz":-180,"elapsed":332,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["14"]},"metadata":{},"execution_count":10}],"source":["sequence = RunnableLambda(lambda x: x + 1) | {\n","    'mul_2': RunnableLambda(lambda x: x * 2),\n","    'mul_5': RunnableLambda(lambda x: x * 5)\n","} | RunnableLambda(lambda x: x['mul_2'] + x['mul_5'])\n","sequence.invoke(1) # {'mul_2': 4, 'mul_5': 10}"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"qZjurxPvCCt-","outputId":"43f750de-e177-4bce-a838-124f3ffee4b2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727711715990,"user_tz":-180,"elapsed":248,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'langchain_core.runnables.base.RunnableParallel'>\n","<class 'dict'>\n"]}],"source":["from langchain_core.runnables import RunnableParallel\n","\n","parallel = RunnableParallel({\n","    'mul_2': RunnableLambda(lambda x: x * 2),\n","    'mul_5': RunnableLambda(lambda x: x * 5)\n","})\n","\n","# This is a dictionary, however it will be composed with other runnables when used in a sequence:\n","parallel_two = {\n","    'mul_2': RunnableLambda(lambda x: x['input_one'] * 2),\n","    'mul_5': RunnableLambda(lambda x: x['input_two'] * 5)\n","}\n","\n","print(type(parallel)) # <class 'langchain.schema.runnable.RunnableParallel'>\n","print(type(parallel_two)) # <class 'dict'>"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"MeEZCoX5CCt-","outputId":"861115ed-31f5-493c-a0f4-60ec8e905c2e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727711775017,"user_tz":-180,"elapsed":300,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["35"]},"metadata":{},"execution_count":12}],"source":["chain = parallel | RunnableLambda(lambda x: x['mul_2'] + x['mul_5'])\n","chain.invoke(5)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"lqvMYC98CCt-","outputId":"65247490-2df7-49a1-956e-423965b7bb0e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727711850210,"user_tz":-180,"elapsed":244,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["60"]},"metadata":{},"execution_count":13}],"source":["second_chain = parallel_two | RunnableLambda(lambda x: x['mul_2'] + x['mul_5'])\n","second_chain.invoke({'input_one': 5, 'input_two': 10})"]},{"cell_type":"markdown","metadata":{"id":"SjvDeKsECCt-"},"source":["------------------------------------------------\n","\n","### You only need a _`Runnable` at the start_, you can use other Python functions _after the first `Runnable`_\n","\n","Technically you only need a `RunnableLambda` or `RunnableParallel` as the first expression after that you can use Python functions:"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"EcGyarQMCCt-","executionInfo":{"status":"ok","timestamp":1727711883037,"user_tz":-180,"elapsed":334,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["# parallel = RunnableParallel({\n","#     'mul_2': RunnableLambda(lambda x: x * 2),\n","#     'mul_5': RunnableLambda(lambda x: x * 5)\n","# })\n","\n","# # This is bad practice:\n","# test = lambda x : x + 1  | parallel\n","# print(test)\n","# test.invoke(5)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"SsezfHjlCCt-","outputId":"935c2344-553b-4f76-9fe3-22bdd7053b14","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727711890395,"user_tz":-180,"elapsed":347,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["first=RunnableLambda(lambda x: x + 1) middle=[] last={\n","  mul_2: RunnableLambda(...),\n","  mul_5: RunnableLambda(...)\n","}\n"]},{"output_type":"execute_result","data":{"text/plain":["{'mul_2': 12, 'mul_5': 30}"]},"metadata":{},"execution_count":16}],"source":["# This is good practice:\n","test = RunnableLambda(lambda x: x + 1) | parallel\n","print(test)\n","test.invoke(5)"]},{"cell_type":"markdown","metadata":{"id":"tDWeIiv3CCt-"},"source":["---\n","\n","## Combining Steps in A Runnable"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"m3KA7nWQCCt-","executionInfo":{"status":"ok","timestamp":1727711905754,"user_tz":-180,"elapsed":292,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["from langchain_core.runnables import RunnableParallel\n","\n","parallel = RunnableParallel({\n","    'item_one': RunnableLambda(lambda x: f\"Hello {x['name']} \"),\n","    'item_two': RunnableLambda(lambda x: 'Welcome to the World!')\n","})\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"xtuGuLAbCCt-","executionInfo":{"status":"ok","timestamp":1727711933477,"user_tz":-180,"elapsed":230,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["def combine(x):\n","    return x['item_one'] + x['item_two']"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"nkpMDuvtCCt-","outputId":"5d37fe62-fb0e-458b-b33e-da7a9af31f47","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1727712039820,"user_tz":-180,"elapsed":273,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Hello James Welcome to the World!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}],"source":["parallel_chain_example = parallel | combine\n","parallel_chain_example.invoke({'name': \"James\"})"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"6JPKqSzrCCt-","outputId":"3eaf3bb1-d30d-4b47-94cb-17918a0bc743","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1727712044116,"user_tz":-180,"elapsed":474,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Hello World'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}],"source":["lambda_example = RunnableLambda(lambda x: {'item_one': 'Hello ', 'item_two': 'World'})\n","lambda_chain_example = lambda_example | combine\n","lambda_chain_example.invoke({})"]},{"cell_type":"code","source":[],"metadata":{"id":"IXWLUfp8qarZ"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}