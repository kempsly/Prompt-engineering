{"cells":[{"cell_type":"markdown","metadata":{"id":"qSPeQyoHB5Cm"},"source":["# LangChain Expression Language"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"umZSjMxlB5Cn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727717809533,"user_tz":-180,"elapsed":11691,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"8696b2c3-32d9-4a9f-82da-464c355e989d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n","Collecting langchain_openai\n","  Downloading langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting langchain-core<0.4.0,>=0.3.6 (from langchain)\n","  Downloading langchain_core-0.3.6-py3-none-any.whl.metadata (6.3 kB)\n","Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n","  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n","  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n","Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n","  Downloading openai-1.50.2-py3-none-any.whl.metadata (24 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain_openai)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.6->langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.12.2)\n","Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n","Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n","  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain)\n","  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n","Downloading langchain-0.3.1-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_openai-0.2.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.6-py3-none-any.whl (399 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n","Downloading langsmith-0.1.129-py3-none-any.whl (292 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.50.2-py3-none-any.whl (382 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.0/383.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n","Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, jiter, h11, faiss-cpu, tiktoken, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langchain-text-splitters, langchain_openai, langchain\n","  Attempting uninstall: tenacity\n","    Found existing installation: tenacity 9.0.0\n","    Uninstalling tenacity-9.0.0:\n","      Successfully uninstalled tenacity-9.0.0\n","Successfully installed faiss-cpu-1.8.0.post1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.1 langchain-core-0.3.6 langchain-text-splitters-0.3.0 langchain_openai-0.2.1 langsmith-0.1.129 openai-1.50.2 orjson-3.10.7 tenacity-8.5.0 tiktoken-0.7.0\n"]}],"source":["%pip install langchain faiss-cpu langchain_openai --upgrade"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"mktviBByB5Co","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727717817524,"user_tz":-180,"elapsed":3954,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"af8b8fce-47e5-4eab-8517-6b0f35ffa3be"},"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}],"source":["import getpass\n","import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"]},{"cell_type":"code","source":["!pip install langchain_community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qk4yL_3hAfa_","executionInfo":{"status":"ok","timestamp":1727717847134,"user_tz":-180,"elapsed":6058,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"28dafbc2-3379-46ce-9e89-d2a67447ff8a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain_community\n","  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.5)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: langchain<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.1)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.6)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.129)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n","  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->langchain_community) (0.3.0)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->langchain_community) (2.9.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.7)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.5)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain_community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (2.23.4)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n","Downloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n","Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n","Successfully installed dataclasses-json-0.6.7 langchain_community-0.3.1 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 python-dotenv-1.0.1 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"MndBYaNvB5Co","executionInfo":{"status":"ok","timestamp":1727717852122,"user_tz":-180,"elapsed":212,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["from langchain_core.runnables import RunnableMap, RunnablePassthrough, RunnableLambda\n","from langchain_core.prompts import format_document\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts.chat import ChatPromptTemplate\n","from langchain_openai.chat_models import ChatOpenAI\n","from langchain_core.prompts.prompt import PromptTemplate\n","from operator import itemgetter\n","from langchain_community.vectorstores.faiss import FAISS\n","from langchain_openai import OpenAIEmbeddings"]},{"cell_type":"markdown","metadata":{"id":"6S1_ask1B5Cp"},"source":["---\n","\n","## Adding In Conversational History:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2XLoxQbmB5Cp","executionInfo":{"status":"ok","timestamp":1727717887645,"user_tz":-180,"elapsed":1362,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["vectorstore = FAISS.from_texts(\n","    [\n","        \"James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData\",\n","        \"James is 31 years old.\",\n","    ],\n","    embedding=OpenAIEmbeddings(),\n",")\n","retriever = vectorstore.as_retriever()"]},{"cell_type":"markdown","source":["In LangChain, **`PromptTemplate`** and **`ChatPromptTemplate`** serve to structure prompts for language model interactions, but they cater to different use cases. Here’s a detailed comparison of the two:\n","\n","### PromptTemplate\n","\n","- **Definition**:\n","  - `PromptTemplate` is a class used to create general templates for prompts that can be used with various language model tasks. It's not specifically designed for chat applications.\n","\n","- **Use Cases**:\n","  - It is suitable for tasks like text generation, summarization, classification, etc., where the interaction might not necessarily be conversational.\n","  \n","- **Features**:\n","  - **Placeholders**: Supports placeholders for dynamic content (e.g., `{variable_name}`) to customize the prompt based on the context.\n","  - **Formatting**: Provides methods to format the prompt with the specified variables.\n","  - **Static Content**: Can include static text alongside dynamic placeholders.\n","\n","- **Example**:\n","  ```python\n","  from langchain.prompts import PromptTemplate\n","\n","  template = PromptTemplate(\n","      input_variables=[\"text\"],\n","      template=\"Translate the following text to French: {text}\"\n","  )\n","\n","  prompt = template.format(text=\"Hello, how are you?\")\n","  ```\n","\n","### ChatPromptTemplate\n","\n","- **Definition**:\n","  - `ChatPromptTemplate` is a specialized subclass of `PromptTemplate` designed specifically for chat-based interactions. It helps manage prompts within a conversational context.\n","\n","- **Use Cases**:\n","  - Ideal for developing chatbots or conversational agents, where the context of previous messages and user intent is crucial for generating appropriate responses.\n","\n","- **Features**:\n","  - **Role Management**: It can define roles for messages (e.g., `user`, `assistant`), making it easier to format and structure chat interactions.\n","  - **Multi-turn Conversations**: Facilitates the creation of prompts that account for dialogue history, enabling more coherent multi-turn conversations.\n","  - **Easier Context Handling**: Simplifies the process of maintaining context in conversations, which can be challenging with standard prompts.\n","\n","- **Example**:\n","  ```python\n","  from langchain.prompts import ChatPromptTemplate\n","\n","  chat_template = ChatPromptTemplate.from_messages([\n","      (\"user\", \"What is your name?\"),\n","      (\"assistant\", \"I am your AI assistant. How can I help you?\")\n","  ])\n","\n","  prompt = chat_template.format(user_input=\"Can you tell me a joke?\")\n","  ```\n","\n","### Key Differences\n","\n","1. **Purpose**:\n","   - **PromptTemplate**: General-purpose for various prompt-based tasks.\n","   - **ChatPromptTemplate**: Specifically designed for managing chat interactions.\n","\n","2. **Structure**:\n","   - **PromptTemplate**: Focuses on formatting text with placeholders without specific handling for dialogue flow.\n","   - **ChatPromptTemplate**: Includes role-based message structures and is built to handle conversational context.\n","\n","3. **Complexity**:\n","   - **PromptTemplate**: Simpler and more flexible, suitable for a wide range of applications.\n","   - **ChatPromptTemplate**: More complex due to its focus on chat dynamics and context management.\n","\n","### Conclusion\n","In summary, use **`PromptTemplate`** for general prompt creation tasks where conversation dynamics are not critical. Choose **`ChatPromptTemplate`** when developing chat applications that require managing roles and conversational context effectively. This distinction helps streamline the development of applications in LangChain, allowing for better handling of user interactions."],"metadata":{"id":"HWLtmAjlDGCF"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"DKp6kC03B5Cp","executionInfo":{"status":"ok","timestamp":1727717932826,"user_tz":-180,"elapsed":210,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n","\n","Chat History:\n","{chat_history}\n","Follow Up Input: {question}\n","Standalone question:\"\"\"\n","CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"YOZaxbPjB5Cp","executionInfo":{"status":"ok","timestamp":1727717992503,"user_tz":-180,"elapsed":216,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"RXOewC8-B5Cp","executionInfo":{"status":"ok","timestamp":1727718042547,"user_tz":-180,"elapsed":238,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n","\n","def _combine_documents(\n","    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n","):\n","    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n","    return document_separator.join(doc_strings)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"oqHYfopuB5Cp","executionInfo":{"status":"ok","timestamp":1727718451244,"user_tz":-180,"elapsed":220,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["from typing import List, Union\n","from langchain.schema import HumanMessage, SystemMessage, AIMessage\n","\n","def _format_chat_history(chat_history: List[Union[HumanMessage, SystemMessage, AIMessage]]) -> str:\n","    buffer = \"\"\n","    for dialogue_turn in chat_history:\n","        if isinstance(dialogue_turn, HumanMessage):\n","            buffer += \"\\nHuman: \" + dialogue_turn.content\n","        elif isinstance(dialogue_turn, AIMessage):\n","            buffer += \"\\nAssistant: \" + dialogue_turn.content\n","        elif isinstance(dialogue_turn, SystemMessage):\n","            buffer += \"\\nSystem: \" + dialogue_turn.content\n","    return buffer"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"I5gMEhzJB5Cq","executionInfo":{"status":"ok","timestamp":1727718599366,"user_tz":-180,"elapsed":451,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["_inputs = RunnableMap(\n","    standalone_question=RunnablePassthrough.assign(\n","        chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n","    )\n","    | CONDENSE_QUESTION_PROMPT\n","    | ChatOpenAI(temperature=0)\n","    | StrOutputParser(),\n",")\n","_context = {\n","    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n","    \"question\": lambda x: x[\"standalone_question\"],\n","}\n","conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()"]},{"cell_type":"markdown","source":["The code snippet demonstrates a typical usage of **`RunnableMap`** in LangChain for building a conversational question-answering (QA) chain.\n","\n","### Breakdown of the Code\n","\n","1. **Inputs Definition**:\n","   ```python\n","   _inputs = RunnableMap(\n","       standalone_question=RunnablePassthrough.assign(\n","           chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n","       )\n","       | CONDENSE_QUESTION_PROMPT\n","       | ChatOpenAI(temperature=0)\n","       | StrOutputParser(),\n","   )\n","   ```\n","\n","   - **`RunnableMap`**: This creates a mapping for inputs where `standalone_question` is a key. The value is a series of runnables connected by the `|` operator.\n","   - **`RunnablePassthrough`**: This allows the input to be passed through while transforming the `chat_history` using the `_format_chat_history` function. This is useful for preprocessing the chat history before it is used in subsequent steps.\n","   - **`CONDENSE_QUESTION_PROMPT`**: This likely represents a prompt template or a callable that condenses the user's question based on the provided chat history.\n","   - **`ChatOpenAI(temperature=0)`**: This invokes the OpenAI chat model with a temperature of 0, which means the responses will be more deterministic and focused.\n","   - **`StrOutputParser()`**: This processes the output of the chat model, likely converting it into a string format for further use.\n","\n","2. **Context Definition**:\n","   ```python\n","   _context = {\n","       \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n","       \"question\": lambda x: x[\"standalone_question\"],\n","   }\n","   ```\n","\n","   - This creates a context dictionary that defines how to fetch and format the context for the question-answering process.\n","   - **`itemgetter(\"standalone_question\")`**: This retrieves the `standalone_question` output from the previous step.\n","   - **`retriever`**: This component likely retrieves relevant documents or information based on the provided question.\n","   - **`_combine_documents`**: This combines the retrieved documents into a format suitable for the QA chain.\n","\n","3. **Conversational QA Chain**:\n","   ```python\n","   conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()\n","   ```\n","\n","   - This constructs the full conversational QA chain by chaining together the inputs, context, and the answering process.\n","   - **`ANSWER_PROMPT`**: This is likely another prompt template that prepares the input for the OpenAI model based on the context and question.\n","   - **`ChatOpenAI()`**: This invokes the OpenAI model again to generate an answer based on the formatted input from the previous steps.\n","\n","### Explanation of the Workflow\n","\n","1. **Input Handling**: The workflow begins with the user's question and chat history. The `RunnablePassthrough` allows for easy preprocessing of the input, specifically formatting the chat history.\n","\n","2. **Condensing the Question**: The question is then condensed (possibly to enhance clarity or focus) before being processed by the OpenAI model to generate a response.\n","\n","3. **Context Creation**: The context for answering the question is built by retrieving relevant documents based on the condensed question. The context includes the processed standalone question and additional relevant documents.\n","\n","4. **Answer Generation**: Finally, the concatenated inputs (including the context and question) are fed into the OpenAI model again to generate a final answer.\n","\n","\n","This code snippet showcases how to build a structured conversational QA chain using **`RunnableMap`** in LangChain. The modularity and clarity of the approach make it easy to adjust individual components, such as changing how the question is condensed or how the context is retrieved. This modular design allows for more manageable and maintainable code, especially in applications involving complex workflows."],"metadata":{"id":"Wkb7cUebEg34"}},{"cell_type":"markdown","source":["In LangChain, **`RunnableMap`** is a utility designed to help manage and execute a collection of \"runnable\" components, which can be functions, chains, or other callable objects. It allows you to define a mapping between input keys and the corresponding runnable components that will process those inputs. This makes it easier to orchestrate complex workflows that involve multiple processing steps.\n","\n","### Key Features of RunnableMap\n","\n","1. **Input-Output Mapping**:\n","   - You can define a dictionary that maps input keys to runnable components. Each runnable can process its corresponding input independently.\n","\n","2. **Batch Processing**:\n","   - It can handle batch inputs efficiently, allowing you to process multiple inputs in a single invocation.\n","\n","3. **Integration with LangChain**:\n","   - It seamlessly integrates with other components in LangChain, such as chains and agents, making it versatile for various use cases.\n","\n","4. **Parallel Execution**:\n","   - Runnable components can be executed in parallel, improving efficiency when dealing with multiple tasks.\n","\n","5. **Dynamic Composition**:\n","   - You can dynamically compose workflows by adding or modifying runnables based on your needs.\n","\n","### Basic Usage\n","\n","Here's a basic example to illustrate how to use `RunnableMap`:\n","\n","```python\n","from langchain.runnables import RunnableMap, RunnableLambda\n","\n","# Define some sample runnables\n","runnable_a = RunnableLambda(lambda x: x + \" processed by A\")\n","runnable_b = RunnableLambda(lambda x: x + \" processed by B\")\n","\n","# Create a RunnableMap\n","runnable_map = RunnableMap({\n","    \"output_a\": runnable_a,\n","    \"output_b\": runnable_b\n","})\n","\n","# Define the input data\n","inputs = {\n","    \"input_a\": \"Data A\",\n","    \"input_b\": \"Data B\"\n","}\n","\n","# Execute the RunnableMap with the inputs\n","outputs = runnable_map.invoke(inputs)\n","\n","# Print the results\n","print(outputs)\n","```\n","\n","### Explanation of the Example\n","\n","1. **Runnables**:\n","   - Two runnables (`runnable_a` and `runnable_b`) are created using `RunnableLambda`, which applies a simple transformation to the input string.\n","\n","2. **RunnableMap**:\n","   - A `RunnableMap` is instantiated with a dictionary that maps output keys (`\"output_a\"` and `\"output_b\"`) to the corresponding runnables.\n","\n","3. **Input Data**:\n","   - A dictionary of inputs (`inputs`) is defined, containing data for each runnable.\n","\n","4. **Execution**:\n","   - The `invoke` method is called on the `runnable_map` with the input data, which processes each input through its corresponding runnable.\n","\n","5. **Output**:\n","   - The output is a dictionary where each key corresponds to the output from its respective runnable.\n","\n","### Use Cases\n","\n","- **Workflow Management**: Organize complex workflows that require multiple processing steps in a structured manner.\n","- **Parallel Processing**: Efficiently handle tasks that can be processed independently, such as data preprocessing or feature extraction.\n","- **Dynamic Composition**: Build flexible and dynamic workflows that can adapt based on changing requirements.\n","\n","### Conclusion\n","\n","`RunnableMap` is a powerful tool in LangChain for orchestrating complex workflows involving multiple runnables. By defining input-output mappings and enabling batch processing, it helps streamline the development of applications that require coordinated processing of data through various components."],"metadata":{"id":"-q5h16JGD5yn"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"P-jvemZsB5Cq","outputId":"bb0b3aeb-82d5-4cb3-db97-eadf2e028fb3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727718835632,"user_tz":-180,"elapsed":1557,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='James worked at JustUnderstandingData.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 47, 'total_tokens': 54, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-26148b1f-1d06-44ab-b392-f7be748d367d-0', usage_metadata={'input_tokens': 47, 'output_tokens': 7, 'total_tokens': 54})"]},"metadata":{},"execution_count":12}],"source":["conversational_qa_chain.invoke(\n","    {\n","        \"question\": \"where did James work?\",\n","        \"chat_history\": [],\n","    }\n",")"]},{"cell_type":"markdown","metadata":{"id":"ALr_zWBnB5Cq"},"source":["---\n","\n","## Adding Memory"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"xoGFgbTMB5Cq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727718845489,"user_tz":-180,"elapsed":234,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"eadb10d8-f4a6-4e3f-ede0-eef5e134f39a"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-13-8d2c383a96f0>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationBufferMemory(\n"]}],"source":["from operator import itemgetter\n","from langchain.memory import ConversationBufferMemory\n","\n","memory = ConversationBufferMemory(\n","    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"uOjgaIpKB5Cr","executionInfo":{"status":"ok","timestamp":1727718860915,"user_tz":-180,"elapsed":461,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["# First we add a step to load memory\n","# This adds a \"memory\" key to the input object\n","loaded_memory = RunnablePassthrough.assign(\n","    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",")\n","# Now we calculate the standalone question\n","standalone_question = {\n","    \"standalone_question\": {\n","        \"question\": lambda x: x[\"question\"],\n","        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n","    }\n","    | CONDENSE_QUESTION_PROMPT\n","    | ChatOpenAI(temperature=0)\n","    | StrOutputParser(),\n","}\n","# Now we retrieve the documents\n","\n","# This is REALLY IMPORTANT as the chain above becomes StrOutputParser() so it will only have one key, which gets passed to the retriever!\n","retrieved_documents = {\n","    \"docs\": itemgetter(\"standalone_question\") | retriever,\n","    \"question\": lambda x: x[\"standalone_question\"],\n","}\n","# Now we construct the inputs for the final prompt\n","final_inputs = {\n","    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n","    \"question\": itemgetter(\"question\"),\n","}\n","# And finally, we do the part that returns the answers\n","answer = {\n","    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n","    \"docs\": itemgetter(\"docs\"),\n","}\n","# And now we put it all together!\n","final_chain = loaded_memory | standalone_question | retrieved_documents | answer"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"UbP7xqB-B5Cr","outputId":"91e0d888-33a7-472f-d26c-66320ab56a77","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727719005926,"user_tz":-180,"elapsed":1275,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'answer': AIMessage(content='James Phoenix worked at JustUnderstandingData.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 48, 'total_tokens': 56, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-af66e585-676f-4983-b37f-e966de730d30-0', usage_metadata={'input_tokens': 48, 'output_tokens': 8, 'total_tokens': 56}), 'docs': [Document(metadata={}, page_content='James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData'), Document(metadata={}, page_content='James is 31 years old.')]}\n"]}],"source":["inputs = {\"question\": \"where did James Phoenix work?\"}\n","result = final_chain.invoke(inputs)\n","print(result)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"w2baZrAzB5Cr","executionInfo":{"status":"ok","timestamp":1727719016652,"user_tz":-180,"elapsed":260,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["# Note that the memory does not save automatically\n","# This will be improved in the future\n","# For now you need to save it yourself\n","memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"WbuUh3TPB5Cr","outputId":"35883089-819f-4790-c318-1d493acd0cf1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727719021604,"user_tz":-180,"elapsed":248,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': [HumanMessage(content='where did James Phoenix work?', additional_kwargs={}, response_metadata={}),\n","  AIMessage(content='James Phoenix worked at JustUnderstandingData.', additional_kwargs={}, response_metadata={})]}"]},"metadata":{},"execution_count":17}],"source":["memory.load_memory_variables({})"]},{"cell_type":"markdown","metadata":{"id":"xVRMyA4LB5Cr"},"source":["------------------------------------------"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}