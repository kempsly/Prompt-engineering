{"cells":[{"cell_type":"markdown","metadata":{"id":"Bv2D-jL5B-CZ"},"source":["# LangChain Expression Language"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kzaY-p_-B-Ca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727712703549,"user_tz":-180,"elapsed":8968,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"1c7a34ec-1f7c-4a09-d7a4-fc17e963270b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain_openai\n","  Downloading langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting langchain-core<0.4.0,>=0.3.6 (from langchain)\n","  Downloading langchain_core-0.3.6-py3-none-any.whl.metadata (6.3 kB)\n","Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n","  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n","  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n","Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n","  Downloading openai-1.50.2-py3-none-any.whl.metadata (24 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain_openai)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.6->langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.12.2)\n","Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n","Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n","  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain)\n","  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n","Downloading langchain-0.3.1-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_openai-0.2.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.6-py3-none-any.whl (399 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n","Downloading langsmith-0.1.129-py3-none-any.whl (292 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.50.2-py3-none-any.whl (382 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.0/383.0 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n","Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, jiter, h11, tiktoken, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langchain-text-splitters, langchain_openai, langchain\n","  Attempting uninstall: tenacity\n","    Found existing installation: tenacity 9.0.0\n","    Uninstalling tenacity-9.0.0:\n","      Successfully uninstalled tenacity-9.0.0\n","Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.1 langchain-core-0.3.6 langchain-text-splitters-0.3.0 langchain_openai-0.2.1 langsmith-0.1.129 openai-1.50.2 orjson-3.10.7 tenacity-8.5.0 tiktoken-0.7.0\n"]}],"source":["%pip install langchain langchain_openai --upgrade"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wesfSdRJB-Ca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727712721927,"user_tz":-180,"elapsed":7395,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"d543d7ef-9ec2-47c0-9394-0408b7053efd"},"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}],"source":["import getpass\n","import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"s-I9SzTMB-Ca","executionInfo":{"status":"ok","timestamp":1727712737057,"user_tz":-180,"elapsed":740,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["from langchain_core.runnables import (\n","    RunnablePassthrough,\n","    RunnableParallel,\n","    RunnableLambda,\n",")"]},{"cell_type":"markdown","metadata":{"id":"uoORFwpoB-Ca"},"source":["---\n","\n","## Accessing Previous Values using RunnablePassThrough\n","\n","A runnable to passthrough inputs unchanged or with additional keys.\n","\n","This runnable behaves almost like the identity function, except that it can be configured to add additional keys to the output, if the input is a dict.\n","\n","The examples below demonstrate this runnable works using a few simple chains. The chains rely on simple lambdas to make the examples easy to execute and experiment with."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_QecrSYQB-Cb","outputId":"0e24b4dc-074a-400c-f83b-f5b01289085e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727712772303,"user_tz":-180,"elapsed":520,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'origin': 1, 'modified': 2}\n"]},{"output_type":"execute_result","data":{"text/plain":["{'original': 'hello world', 'parsed': 'dlrow olleh'}"]},"metadata":{},"execution_count":4}],"source":["runnable = RunnableParallel(\n","    origin=RunnablePassthrough(),\n","    modified=lambda x: x+1\n",")\n","\n","print(runnable.invoke(1)) # {'origin': 1, 'modified': 2}\n","\n","\n","def fake_llm(prompt: str) -> str: # Fake LLM for the example\n","    return prompt + \" world\"\n","\n","chain = RunnableLambda(fake_llm) | {\n","    'original': RunnablePassthrough(), # Original LLM output\n","    'parsed': lambda text: text[::-1] # Parsing logic\n","}\n","\n","chain.invoke('hello')"]},{"cell_type":"markdown","metadata":{"id":"MO3mlD9-B-Cb"},"source":["---\n","\n","## Prompt + Model"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"e_g7S7t_B-Cb","outputId":"b18fca2a-5394-49e7-d2d5-973c43453453","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727712900224,"user_tz":-180,"elapsed":1180,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["first=ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}'), additional_kwargs={})]) middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x78fe141434c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x78fe141892a0>, root_client=<openai.OpenAI object at 0x78fe1577b190>, root_async_client=<openai.AsyncOpenAI object at 0x78fe141434f0>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n"]}],"source":["from langchain_openai.chat_models import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","chat = ChatOpenAI()\n","prompt = ChatPromptTemplate.from_template('Tell me a joke about {topic}')\n","\n","chain = prompt | chat\n","print(chain)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"o1TNxG61B-Cb","outputId":"cebbae68-81be-4c07-9ddf-bf678b0614f5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727712982553,"user_tz":-180,"elapsed":244,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["first input_variables=['topic'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}'), additional_kwargs={})]\n","last client=<openai.resources.chat.completions.Completions object at 0x78fe141434c0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x78fe141892a0> root_client=<openai.OpenAI object at 0x78fe1577b190> root_async_client=<openai.AsyncOpenAI object at 0x78fe141434f0> model_kwargs={} openai_api_key=SecretStr('**********')\n"]}],"source":["print(\"first\", chain.first)\n","print(\"last\", chain.last)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ekXdXpYzB-Cb","outputId":"87145b3e-2a63-47d1-966a-99eeec1fb1f5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727712993775,"user_tz":-180,"elapsed":2417,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Stream:\n","\n","Why did the bear dissolve in water?\n","\n","Because it was polar!\n","\n","Invoke:\n","\n","Why did the bear break up with his girlfriend? \n","\n","Because he couldn't bear the relationship any longer!\n","\n","\n","Batch:\n","\n","[AIMessage(content=\"Why did the bear break up with his girlfriend? \\n\\nBecause he couldn't bear the relationship any longer!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 13, 'total_tokens': 34, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c4a387af-ac64-498c-84dd-e41431ae3fb6-0', usage_metadata={'input_tokens': 13, 'output_tokens': 21, 'total_tokens': 34}), AIMessage(content=\"Why did the bear break up with his girlfriend? \\n\\nBecause he couldn't bear the relationship any longer!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 13, 'total_tokens': 34, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-66a1201c-fae3-42f0-a367-3820c37e3e92-0', usage_metadata={'input_tokens': 13, 'output_tokens': 21, 'total_tokens': 34}), AIMessage(content=\"Why did the bear break up with his girlfriend?\\nBecause he couldn't bear the relationship anymore!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 13, 'total_tokens': 32, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e8503a7f-88f0-4fb2-9c86-62ef84ea2ede-0', usage_metadata={'input_tokens': 13, 'output_tokens': 19, 'total_tokens': 32})]\n"]}],"source":["# Stream:\n","print('\\n\\nStream:\\n')\n","for s in chain.stream({\"topic\": \"bears\"}):\n","    print(s.content, end=\"\", flush=True)\n","\n","# Invoke:\n","print('\\n\\nInvoke:\\n')\n","print(chain.invoke({\"topic\": \"bears\"}).content)\n","\n","# Batch:\n","print('\\n\\nBatch:\\n')\n","print(chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"bears\"}, {\"topic\": \"bears\"}]))"]},{"cell_type":"markdown","metadata":{"id":"8ToxbtXzB-Cb"},"source":["---\n","\n","## Retrieval Augmented Generation (RAG) in LCEL"]},{"cell_type":"markdown","source":["**Retrieval Augmented Generation (RAG)** is a powerful technique that combines information retrieval and generative models to improve the quality of generated text, especially in cases where the model doesn't have enough specific knowledge. RAG enables language models to answer queries or generate text based on external data sources, such as documents or knowledge bases, by retrieving relevant information before generating the final response.\n","\n","In the context of **LangChain Expression Language (LCEL)**, RAG can be implemented by chaining together components that handle information retrieval and text generation using LCEL's **runnables** and **pipe operators** (`|`).\n","\n","Here’s an outline of how RAG can be built using LCEL:\n","\n","### Key Components for RAG in LCEL\n","\n","1. **Retriever**:\n","   - A module that retrieves relevant documents or information from an external knowledge source (e.g., databases, search engines, or vector stores).\n","   \n","2. **Generative Model**:\n","   - A large language model (LLM) such as GPT-3 or similar, which takes the retrieved information as context to generate the final answer or output.\n","   \n","3. **Pipeline (RunnableSequence)**:\n","   - A chain of operations where the query is passed through a retriever to get relevant documents, which are then combined with the generative model for output generation.\n","\n","### Example of RAG in LangChain using LCEL\n","\n","Let’s build a simple RAG pipeline using LangChain’s runnable components for retrieval and generation. Here’s how you can do that step by step.\n","\n","#### Step 1: Import Necessary Libraries\n","You will need to import the necessary `runnable` and retriever components from LangChain.\n","\n","```python\n","from langchain.schema.runnable import RunnableSequence, RunnableLambda\n","from langchain.llms import OpenAI\n","from langchain.chains.retrieval_qa.base import RetrievalQA\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","```\n","\n","#### Step 2: Set up the Retriever\n","Assume you have a **vector store** (such as FAISS) that stores document embeddings. You need a retriever to query the vector store and fetch relevant documents based on the user input.\n","\n","```python\n","# Load embeddings and initialize a FAISS vector store for retrieval\n","embeddings = OpenAIEmbeddings()\n","vectorstore = FAISS.load_local(\"path_to_your_vectorstore\", embeddings)\n","\n","# Create a retriever from the vector store\n","retriever = vectorstore.as_retriever(search_type=\"similarity\", k=5)\n","```\n","\n","#### Step 3: Set up the Language Model\n","We will use a pre-trained language model (e.g., OpenAI’s GPT-3) for the generative part of the pipeline.\n","\n","```python\n","# Load a language model (GPT-3) from OpenAI\n","llm = OpenAI(model=\"gpt-3.5-turbo\")\n","```\n","\n","#### Step 4: Create the RAG Pipeline with LCEL\n","Now we combine the retrieval and generation steps using **LCEL** by defining a pipeline that first retrieves relevant documents and then generates a response using the language model.\n","\n","```python\n","# Define the RAG pipeline using LCEL\n","rag_pipeline = (\n","    RunnableLambda(lambda query: retriever.get_relevant_documents(query))  # Retrieval step\n","    | RunnableLambda(lambda docs: \" \".join([doc.page_content for doc in docs]))  # Combine docs\n","    | llm.run  # Pass the combined documents to the language model\n",")\n","\n","# Use the RAG pipeline to generate a response\n","query = \"What is the capital of France?\"\n","response = rag_pipeline.invoke(query)\n","\n","print(response)\n","```\n","\n","#### Step 5: Handling Multiple Queries with Batch Processing\n","To process multiple queries, you can use the `batch` method, which allows you to process multiple inputs in parallel.\n","\n","```python\n","# Process multiple queries at once\n","queries = [\"What is the capital of France?\", \"Explain the theory of relativity.\"]\n","responses = rag_pipeline.batch(queries)\n","\n","print(responses)\n","```\n","\n","### Explanation of the RAG Pipeline\n","\n","1. **Retriever**: The retriever gets the most relevant documents from the vector store for the given query. This is done by comparing the query's embedding with the document embeddings stored in the vector store.\n","   \n","2. **Document Combination**: The retrieved documents are combined into a single string, which serves as the context for the language model. This string is created by joining the text of the top `k` relevant documents.\n","   \n","3. **Generative Model**: The combined text is passed to the language model (e.g., GPT-3), which generates a response based on both the original query and the additional context from the retrieved documents.\n","\n","### Advantages of RAG in LCEL\n","\n","- **Knowledge Extension**: RAG allows models to access external information, effectively extending the knowledge base beyond what the model was trained on.\n","  \n","- **Customizable Pipelines**: With LCEL, you can easily modify the pipeline to fit different tasks by swapping out components (e.g., changing the retriever or the language model).\n","  \n","- **Scalability**: LCEL enables scalable batch processing of multiple queries, making it easier to handle large datasets or a high volume of requests.\n","  \n","- **Modular and Composable**: Each step in the RAG pipeline is modular, so you can compose different workflows for retrieval and generation depending on the use case.\n","\n","### Further Enhancements\n","\n","- **Contextual Preprocessing**: You can add additional preprocessing steps (like reformatting the query) before retrieval using LCEL.\n","  \n","- **Post-Processing**: You can introduce post-processing steps (like summarization or answer extraction) after generation.\n","  \n","- **Advanced Retrieval**: Incorporate more sophisticated retrievers, such as hybrid retrieval (combining keyword search with embeddings) or re-ranking techniques.\n","\n","### Full Example with Pre/Post Processing\n","\n","```python\n","# A more complex RAG pipeline with pre/post-processing\n","rag_pipeline = (\n","    RunnableLambda(lambda query: f\"Find relevant documents for: {query}\")  # Pre-processing step\n","    | RunnableLambda(lambda query: retriever.get_relevant_documents(query))  # Retrieval step\n","    | RunnableLambda(lambda docs: \" \".join([doc.page_content for doc in docs]))  # Combine docs\n","    | llm.run  # Generative model call\n","    | RunnableLambda(lambda response: response.strip())  # Post-processing step\n",")\n","\n","# Generate response for a query\n","response = rag_pipeline.invoke(\"What is quantum computing?\")\n","print(response)\n","```\n","\n","In this pipeline:\n","1. We add a **pre-processing step** to format the query.\n","2. The **retriever** fetches relevant documents.\n","3. The documents are passed to the language model, which generates an answer.\n","4. A **post-processing step** trims any unnecessary white spaces from the final response.\n","\n","### Conclusion\n","\n","RAG in LCEL is a flexible and powerful way to combine information retrieval with language generation. By leveraging the **modularity** and **composability** of LCEL, you can build sophisticated pipelines for various use cases like question answering, summarization, or content generation augmented with external knowledge sources."],"metadata":{"id":"ujJHmcIpvJfe"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"7LpWNLREB-Cb","outputId":"dd2394d6-24e2-4628-8bea-56003b2065ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727713053941,"user_tz":-180,"elapsed":7974,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install langchain openai faiss-cpu tiktoken langchain-community --upgrade --quiet"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vV30YYP6B-Cb","executionInfo":{"status":"ok","timestamp":1727713079504,"user_tz":-180,"elapsed":229,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["from operator import itemgetter\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai.chat_models import ChatOpenAI\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_community.vectorstores.faiss import FAISS"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"eBM35GwqB-Cc","executionInfo":{"status":"ok","timestamp":1727713109487,"user_tz":-180,"elapsed":1503,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["vectorstore = FAISS.from_texts(\n","    [\"James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData\", \"James has an age of 31 years old.\"], embedding=OpenAIEmbeddings()\n",")\n","retriever = vectorstore.as_retriever()\n","\n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","model = ChatOpenAI()"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"A_Uve1ivB-Cc","executionInfo":{"status":"ok","timestamp":1727713828196,"user_tz":-180,"elapsed":229,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[],"source":["chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | model\n","    | StrOutputParser()\n",")\n","\n","# It's the same as this, but the tuple allows for line breaks:\n","# {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser()"]},{"cell_type":"code","source":["chain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I1sqOrsAxScc","executionInfo":{"status":"ok","timestamp":1727713850086,"user_tz":-180,"elapsed":238,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"ee23e7e6-efca-42c9-ccb8-fa61cc361871"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{\n","  context: VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x78fe07534e80>, search_kwargs={}),\n","  question: RunnablePassthrough()\n","}\n","| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])\n","| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x78fe07536fb0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x78fe075310f0>, root_client=<openai.OpenAI object at 0x78fe07534ee0>, root_async_client=<openai.AsyncOpenAI object at 0x78fe07537010>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n","| StrOutputParser()"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","execution_count":13,"metadata":{"id":"JUCJtbvdB-Cc","outputId":"8c4e873f-867f-42de-83b3-8494329fefa7","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1727713858932,"user_tz":-180,"elapsed":1390,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'James Phoenix works at JustUnderstandingData.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["chain.invoke(\"What company does James phoenix work at?\")"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"DgyX2W-2B-Cc","outputId":"e7172b7f-bd9e-400e-afa6-29d9e78a8ce2","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1727713873677,"user_tz":-180,"elapsed":1145,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'James Phoenix is 31 years old.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["chain.invoke(\"What is James Phoenix's age?\")"]},{"cell_type":"markdown","metadata":{"id":"3W4Bha9XB-Cc"},"source":["---\n","\n","## Understanding How `itemgetter` Works with Piping"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"_FeqI7E0B-Cc","outputId":"70fac23b-73d0-4fe8-fb90-41f9724b51a0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727713888062,"user_tz":-180,"elapsed":254,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["operator.itemgetter({'data': ['This is a test', 'Another entry...']})\n","['This is a test', 'Another entry...']\n"]}],"source":["test = {\n","    \"data\": ['This is a test', 'Another entry...']\n","}\n","\n","print(itemgetter(test))\n","print(itemgetter('data')(test))"]},{"cell_type":"markdown","metadata":{"id":"xH22HLELB-Cc"},"source":["### How does it work within the context of LCEL?"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"S3LYvdePB-Cc","outputId":"a67e5c13-a31c-4f15-a151-35a959cb90b1","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1727713958317,"user_tz":-180,"elapsed":1013,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["''"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}],"source":["prompt = ChatPromptTemplate.from_template('''What is the profession of James Phoenix? His profession is {profession}.''')\n","\n","first_chain = RunnableParallel(\n","    name=lambda x: \"James Phoenix\",\n","    age=lambda x: 31\n",")\n","\n","second_chain = {\n","    # itemgetter is used to get the value from the dictionary from the previous step: (note this is only the previous step, not the whole chain)\n","    'name': itemgetter('name'),\n","    'age': itemgetter('age'),\n","    # You can not use string values, either use itemgetter or a lambda, or RunnablePassthrough\n","    'profession': lambda x: \"Data Engineer\"\n","}\n","\n","chain = first_chain | second_chain |  prompt |  ChatOpenAI() | StrOutputParser()\n","chain.invoke({})"]},{"cell_type":"code","source":[],"metadata":{"id":"M9ucFR_byDzS"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}