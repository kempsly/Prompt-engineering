{"cells":[{"cell_type":"markdown","metadata":{"id":"pl_lzWsT7K6n"},"source":["_Optimizing a Prompt for Production:_\n","# Social Media Posts\n","\n","Task: Write a social post given an insight and a social network.\n","\n","`insight, social_network -> social_post`\n"]},{"cell_type":"code","source":["pip install openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qh0J9aRH7ZeK","executionInfo":{"status":"ok","timestamp":1728389052337,"user_tz":-180,"elapsed":6164,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"7be09788-e23c-41ba-99f1-f4a1ecec75e0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-1.51.1-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting jiter<1,>=0.4.0 (from openai)\n","  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n","Downloading openai-1.51.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n","Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 openai-1.51.1\n"]}]},{"cell_type":"code","source":["import getpass\n","import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IGAboWaN7Zrb","executionInfo":{"status":"ok","timestamp":1728389062242,"user_tz":-180,"elapsed":7819,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"9fa41d4d-a254-48b7-a680-1e776093a6aa"},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}]},{"cell_type":"markdown","metadata":{"id":"zZRNxSWY7K6v"},"source":["### 0. Naive Prompt\n","Start with simple instructions to establish the baseline performance."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7jZTRMFI7K6w","executionInfo":{"status":"ok","timestamp":1728389107148,"user_tz":-180,"elapsed":2435,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"dcdbbcb8-d152-4317-9d00-6b5341271088"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input:\n","- insight: prompt engineering will still be needed with smarter models, as even genius humans need prompting from legal, HR, management to align with business interests\n","- social_network: Twitter\n","\n","Output:\n","🔍✨ Even as AI models become more intelligent, the role of prompt engineering remains crucial! Just like the brightest minds in any company need guidance from legal, HR, and management to align their brilliance with business goals, smarter AI models need the right prompts to unlock their full potential. 🤖💡\n","\n","Prompt engineers are the bridge between raw AI power and strategic business success. They ensure that advanced models deliver results that are on point with company objectives! 🚀\n","\n","Let's continue to shape the future by embracing the art of prompting, just as we guide human genius to achieve harmony and progress. 🌟 #PromptEngineering #AIAlignment #FutureOfWork\n"]}],"source":["import openai\n","\n","def get_completion(prompt, context):\n","    client = openai.OpenAI()\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o\",\n","        messages=[\n","            {\"role\": \"user\", \"content\": prompt.format(**context)}\n","        ],\n","        max_tokens=500\n","    )\n","\n","    return response.choices[0].message.content.strip()\n","\n","naive_prompt = \"Write a social media post about how {insight}, for {social_network}.\"\n","context = {\n","    \"insight\": \"prompt engineering will still be needed with smarter models, as even genius humans need prompting from legal, HR, management to align with business interests\",\n","    \"social_network\": \"Twitter\"\n","}\n","\n","social_post = get_completion(naive_prompt, context)\n","print(\"Input:\")\n","for key, value in context.items():\n","    print(f\"- {key}: {value}\")\n","print(f\"\\nOutput:\\n{social_post}\")\n"]},{"cell_type":"markdown","metadata":{"id":"g8W2Tz1z7K61"},"source":["---\n","Applying the 'Five Principles of Prompting' from [Prompt Engineering for Generative AI](https://www.amazon.com/Prompt-Engineering-Generative-AI-Future-Proof/dp/109815343X/):"]},{"cell_type":"markdown","metadata":{"id":"udxlIWpm7K61"},"source":["### 1. Give Direction\n","Describe the desired style in detail, or reference a relevant persona."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mbl67X3i7K62","executionInfo":{"status":"ok","timestamp":1728389299862,"user_tz":-180,"elapsed":2540,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"99df2520-317b-42cb-f467-b92362f5a40e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Output:\n","In the age of ever-sophisticated AI, here's an unexpected truth: prompt engineering isn't going anywhere. 🤔 Just as genius humans need a nudge from HR or management to align with company goals, smarter AI needs thoughtful prompts to operate at peak efficiency. Remember, even Einstein had to file his taxes. The essence of guidance remains the same, whether you're steering a maverick mind or Silicon Valley's finest algorithm. 🤖💡 The art of asking the right questions is as crucial now as ever. Let's keep the dialogue going and see how far we can push those digital frontiers! #PromptEngineering #AI #FutureIsNow\n"]}],"source":["direction_prompt = \"\"\"\"Write a social media post about how {insight}, for {social_network}, in the style of Malcolm Gladwell.\n","\n","The social post should sound authentically human and colloquial, while being practically useful and brief. Be very creative and make niche references to sound more human. To maximize engagement, the content should be surprising, interesting, or practically useful, or induce high-arousal emotions such as anxiety, anger, awe, or surprise.\"\"\"\n","\n","social_post_a = get_completion(direction_prompt, context)\n","print(f\"\\nOutput:\\n{social_post_a}\")"]},{"cell_type":"markdown","metadata":{"id":"gIN7qZbz7K63"},"source":["### 2. Specify Format\n","Define what rules to follow, and the required structure of the response."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCZsQJF37K64","executionInfo":{"status":"ok","timestamp":1728389452439,"user_tz":-180,"elapsed":4003,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"cc725584-cb96-4382-8f76-e9be9ab0118d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Output:\n","```yaml\n","bait: \"Even Einstein needed prompting—why our smartest models still crave it.\"\n","hook: \"Just like top talent at Google or Amazon need guidance from HR and legal, AI models must align with broader business goals. Ever wondered how?\"\n","reward: \"Prompt engineers are the unsung heroes, ensuring AI's genius aligns with human strategy. It's a little-known fact, but companies could face a 40% efficiency drop without them (Source: McKinsey).\"\n","post_content: \"Even Einstein needed prompting—why our smartest models still crave it. Just like top talent at Google or Amazon require insight from HR and legal to stay aligned with company interests, AI models are no different. Enter the prompt engineer, the unsung hero who guides these digital minds to sync with human strategy. A fascinating yet hidden truth: companies could face a staggering 40% efficiency drop without this crucial role (Source: McKinsey). So next time you think AI can do it all, remember, every genius needs a guide.\"\n","```\n"]}],"source":["format_prompt = \"\"\"\"Write a social media post about how {insight}, for {social_network}, in the style of Malcolm Gladwell, using the Bait, Hook, Reward framework.\n","\n","- **bait**: Grab the attention of the person browsing social media. This could be a contrarian statement, appeal to identity, celebrity name, or anything else that stops them scrolling.\n","- **hook**: Keep their attention now you have it. Show this post is relevant to them, by alerting them to an issue, using familiar words, or providing social proof to keep them reading.\n","- **reward**: Compensate them for their attention to elicit reciprocation. Is there any useful information, a surprising statistic, or an interesting anecdote to reveal? Or a threat to make?\n","- **post_content**: The main content of the post, incorporating the bait, hook, and reward in a way that resonates with the reader and engages their attention.\n","\n","First decide on the bait, hook, and reward, before writing the post_content.\n","\n","## Instructions\n","The social post should sound authentically human and colloquial, while being practically useful and brief. Be very creative and make niche references to sound more human. To maximize engagement, the content should be surprising, interesting, or practically useful, or induce high-arousal emotions such as anxiety, anger, awe, or surprise.\n","\n","Follow best practices for posting engaging content on {social_network}, but do not use emoji or hashtags.\n","Provide citations for any statistics included. Do not reference the work of Malcolm Gladwell.\n","Respond in YAML with the following keys:\n","- bait\n","- hook\n","- reward\n","- post_content\"\"\"\n","\n","social_post_b = get_completion(format_prompt, context)\n","print(f\"\\nOutput:\\n{social_post_b}\")"]},{"cell_type":"markdown","metadata":{"id":"HfZqtA0_7K65"},"source":["### 3. Provide Examples\n","Insert a diverse set of test cases where the task was done correctly."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ILHOvoC7K65","executionInfo":{"status":"ok","timestamp":1728389736719,"user_tz":-180,"elapsed":289,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"159e22ce-bdea-4b5b-a39e-003e0b014526"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example 1:\n","- insight: overqualified students take service jobs\n","- social_network: Facebook\n","- bait: Use the trope that smart people are so focused they fail to look after themselves.\n","- hook: Particle Physics is a subject that is universally associated with smart people.\n","- reward: Fantasizing about being somewhere remote where you can earn money while focusing on your work.\n","- post_content: Meet Jon. He has been working the night shift at our hotel for over a month already, and he says the best part is the peace and quiet. Particle physics, which is the topic of Jon's Masters Thesis, requires concentration. That's fine by us. So long as our guests can count on Jon, he can count all the particles he wants (or whatever it is that particle physicists do).\n","\n","Example 2:\n","- insight: there's no such thing as a wasted trip\n","- social_network: Instagram\n","- bait: Mentioning a 'wasted trip' will get the attention of people who like to travel, and for whom wasting a trip would be upsetting.\n","- hook: Talk about a rainy day in a hot location like Kauai, which most people would complain about because they're wasting time inside.\n","- reward: An anecdote that reveals that good things can happen when you least expect them.\n","- post_content: There's no such thing as a wasted trip! On this rainy day in Kauai 2 years ago I met my bestie @moniqueontour and we've been on 4 amazing adventures together since.\n","\n","Example 3:\n","- insight: mould your business to your own preferences\n","- social_network: LinkedIn\n","- bait: Most self-help advice focuses on fixing weaknesses, but instead let's focus on how to reframe weaknesses into strengths.\n","- hook: We need to make it clear this post is for business owners, and that you are speaking from experience, establishing connection and credibility.\n","- reward: The fact that 60% of new business came from the blog is a statistic they can take away and use as evidence to support their own strategy, or to share with their wider network.\n","- post_content: Don't try to mold yourself into the person you think your business needs you to be, build your business around who you actually are. When I started my agency business, I hated networking. I'm introverted by nature  so I wrote blog posts instead – eventually 60% of my agency's new business came from our blog.\n","\n","Example 4:\n","- insight: whales are in danger and most people don't know\n","- social_network: Twitter\n","- bait: People who care about whales or animals will pay attention when whales are mentioned.\n","- hook: If we refer to the whales as \"in danger\" people who care will want to know more.\n","- reward: Some species of whales are going extinct, and people would benefit from knowing.\n","- post_content: The whales are in danger of extinction. Many people aren't aware, which is why it's so important to bring awareness to this cause. Together with a few colleagues, we'll be running the London marathon next week in aid of the \"Save the Whales\" foundation, and any support would be appreciated.\n","\n","\n"]}],"source":["examples_prompt = \"\"\"\"Write a social media post about how {insight}, for {social_network}, in the style of Malcolm Gladwell, using the Bait, Hook, Reward framework.\n","\n","- **bait**: Grab the attention of the person browsing social media. This could be a contrarian statement, appeal to identity, celebrity name, or anything else that stops them scrolling.\n","- **hook**: Keep their attention now you have it. Show this post is relevant to them, by alerting them to an issue, using familiar words, or providing social proof to keep them reading.\n","- **reward**: Compensate them for their attention to elicit reciprocation. Is there any useful information, a surprising statistic, or an interesting anecdote to reveal? Or a threat to make?\n","- **post_content**: The main content of the post, incorporating the bait, hook, and reward in a way that resonates with the reader and engages their attention.\n","\n","First decide on the bait, hook, and reward, before writing the post_content.\n","\n","## Examples\n","{examples_partial}\n","\n","## Instructions\n","The social post should sound authentically human and colloquial, while being practically useful and brief. Be very creative and make niche references to sound more human. To maximize engagement, the content should be surprising, interesting, or practically useful, or induce high-arousal emotions such as anxiety, anger, awe, or surprise.\n","\n","Follow best practices for posting engaging content on {social_network}, but do not use emoji or hashtags.\n","Provide citations for any statistics included. Do not reference the work of Malcolm Gladwell.\n","Respond in YAML with the following keys:\n","- bait\n","- hook\n","- reward\n","- post_content\"\"\"\n","\n","examples = [\n","    {\n","        \"insight\": \"overqualified students take service jobs\",\n","        \"social_network\": \"Facebook\",\n","        \"bait\": \"Use the trope that smart people are so focused they fail to look after themselves.\",\n","        \"hook\": \"Particle Physics is a subject that is universally associated with smart people.\",\n","        \"reward\": \"Fantasizing about being somewhere remote where you can earn money while focusing on your work.\",\n","        \"post_content\": \"Meet Jon. He has been working the night shift at our hotel for over a month already, and he says the best part is the peace and quiet. Particle physics, which is the topic of Jon's Masters Thesis, requires concentration. That's fine by us. So long as our guests can count on Jon, he can count all the particles he wants (or whatever it is that particle physicists do).\"\n","    },\n","    {\n","        \"insight\": \"there's no such thing as a wasted trip\",\n","        \"social_network\": \"Instagram\",\n","        \"bait\": \"Mentioning a 'wasted trip' will get the attention of people who like to travel, and for whom wasting a trip would be upsetting.\",\n","        \"hook\": \"Talk about a rainy day in a hot location like Kauai, which most people would complain about because they're wasting time inside.\",\n","        \"reward\": \"An anecdote that reveals that good things can happen when you least expect them.\",\n","        \"post_content\": \"There's no such thing as a wasted trip! On this rainy day in Kauai 2 years ago I met my bestie @moniqueontour and we've been on 4 amazing adventures together since.\"\n","    },\n","    {\n","        \"insight\": \"mould your business to your own preferences\",\n","        \"social_network\": \"LinkedIn\",\n","        \"bait\": \"Most self-help advice focuses on fixing weaknesses, but instead let's focus on how to reframe weaknesses into strengths.\",\n","        \"hook\": \"We need to make it clear this post is for business owners, and that you are speaking from experience, establishing connection and credibility.\",\n","        \"reward\": \"The fact that 60% of new business came from the blog is a statistic they can take away and use as evidence to support their own strategy, or to share with their wider network.\",\n","        \"post_content\": \"Don't try to mold yourself into the person you think your business needs you to be, build your business around who you actually are. When I started my agency business, I hated networking. I'm introverted by nature  so I wrote blog posts instead – eventually 60% of my agency's new business came from our blog.\"\n","    },\n","    {\n","        \"insight\": \"whales are in danger and most people don't know\",\n","        \"social_network\": \"Twitter\",\n","        \"bait\": \"People who care about whales or animals will pay attention when whales are mentioned.\",\n","        \"hook\": \"If we refer to the whales as \\\"in danger\\\" people who care will want to know more.\",\n","        \"reward\": \"Some species of whales are going extinct, and people would benefit from knowing.\",\n","        \"post_content\": \"The whales are in danger of extinction. Many people aren't aware, which is why it's so important to bring awareness to this cause. Together with a few colleagues, we'll be running the London marathon next week in aid of the \\\"Save the Whales\\\" foundation, and any support would be appreciated.\"\n","    }\n","]\n","\n","# Convert examples to a partial string\n","examples_partial = \"\"\n","for i, example in enumerate(examples, 1):\n","    examples_partial += f\"Example {i}:\\n\"\n","    for key, value in example.items():\n","        examples_partial += f\"- {key}: {value}\\n\"\n","    examples_partial += \"\\n\"\n","\n","print(examples_partial)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sn2Frb9W7K67","executionInfo":{"status":"ok","timestamp":1728390519097,"user_tz":-180,"elapsed":3959,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"9c6a83d0-5308-45f1-f916-505864db39e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Output:\n","```yaml\n","bait: Even the sharpest minds need a nudge to stay on track.\n","hook: Just like Einstein needed reminders from his colleagues, smarter AI models will still require prompt engineers to steer them toward business goals. \n","reward: Discover how future-ready you can be as an AI whisperer when 68% of businesses in the next decade are projected to use AI tools for decision-making (source: McKinsey).\n","post_content: \"The truth is, even the sharpest minds need a nudge to stay on track. Think of Einstein's colleagues who helped bridge his genius to practical objectives. Now, picture smarter AI models—they'll still lean on prompt engineers to align with businesses. Just as 68% of companies will soon rely on AI for decision-making, imagine being the future-ready AI whisperer they need (source: McKinsey). #AI #FutureOfWork\"\n","```\n"]}],"source":["# Add examples_partial to the context\n","examples_context = {\n","    \"examples_partial\": examples_partial,\n","    \"social_network\": \"Twitter\",\n","    \"insight\": \"prompt engineering will still be needed with smarter models, as even genius humans need prompting from legal, HR, management to align with business interests\"\n","}\n","\n","social_post_c = get_completion(examples_prompt, examples_context)\n","print(f\"\\nOutput:\\n{social_post_c}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mL6s2ofy7K67","executionInfo":{"status":"ok","timestamp":1728390746690,"user_tz":-180,"elapsed":306,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"76e28d17-4b95-4e3b-d72f-6a0e96179a6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Emoji Evaluation:\n","Has emojis: False\n","Emoji count: 0\n","Emojis found: None\n"]}],"source":["# Define a function to check for emojis in the post content\n","def check_for_emojis(post_content):\n","    import re\n","\n","    # Regular expression pattern to match emojis\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U0001F910-\\U0001F93A\"  # Additional range including 🤔 (U+1F914)\n","        \"]+\", flags=re.UNICODE)\n","\n","    # Check if there are any emojis in the post content\n","    emojis_found = emoji_pattern.findall(post_content)\n","\n","    return {\n","        \"has_emojis\": len(emojis_found) > 0,\n","        \"emoji_count\": len(emojis_found),\n","        \"emojis\": emojis_found\n","    }\n","\n","# Evaluate the presence of emojis in the generated social post\n","emoji_evaluation = check_for_emojis(social_post_c)\n","print(\"\\nEmoji Evaluation:\")\n","print(f\"Has emojis: {emoji_evaluation['has_emojis']}\")\n","print(f\"Emoji count: {emoji_evaluation['emoji_count']}\")\n","print(f\"Emojis found: {', '.join(emoji_evaluation['emojis']) if emoji_evaluation['emojis'] else 'None'}\")\n"]},{"cell_type":"markdown","metadata":{"id":"-u52Sx5W7K68"},"source":["### 4. Evaluate Quality\n","Identify errors and rate responses, testing what drives performance."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kaFD3pYA7K68","executionInfo":{"status":"ok","timestamp":1728392978991,"user_tz":-180,"elapsed":4470,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"3bd9c9a6-ce62-4735-cff6-cc11ca767525"},"outputs":[{"output_type":"stream","name":"stdout","text":["Engagement Evaluation:\n","```yaml\n","Analysis: \n","  - Insight: The post builds on the insight that even advanced AI models will require human input and alignment with business interests, which is relevant to ongoing discussions about AI in the workplace.\n","  - Bait: It attempts to grab attention by referencing Einstein, a universally recognized figure, which could pique interest initially.\n","  - Hook: The analogy of Einstein needing guidance may keep the reader's attention as they draw parallels to the necessity of prompt engineering for AI. However, the leap from Einstein to AI might not be directly relatable for everyone.\n","  - Reward: The post suggests value by presenting the opportunity to be an \"AI whisperer,\" appealing to forward-thinking individuals interested in the future of work.\n","  - Social Network: The post is concise and within character limits for Twitter, making effective use of hashtags to tap into trending discussions surrounding AI.\n","  - Hallucination: The statistic \"68% of companies will soon rely on AI for decision-making\" is claimed to be sourced from McKinsey. If this statistic is fabricated or inaccurately represented, it undermines the post's credibility significantly.\n","  - Human: The post portrays an authentic tone, engaging storytelling, and a slightly aspirational message that resonates on a human level.\n","\n","Rating: 0\n","```\n"]}],"source":["# Define a function to evaluate the engagement potential of a social media post\n","def evaluate_engagement(post_content, insight, social_network):\n","    evaluation_prompt = f\"\"\"\n","    You are an expert social media analyst. Your task is to evaluate the following social media post and predict its engagement potential. Consider factors such as:\n","    - Insight: Does the post use the relevant insight?\n","    - Bait: Does it grab attention?\n","    - Hook: Does it keep the attention?\n","    - Reward: Does it compensate for the attention?\n","    - Social Network: Does it follow best practices for the social network?\n","    - Hallucination: Are any statistics accurate?\n","    - Human: Is the post authentically human-sounding?\n","\n","    Insight: {insight}\n","    Social Network: {social_network}\n","    Post content:\n","    \"{post_content}\"\n","\n","    Based on these factors, rate the post's engagement potential on a scale of 1-5, where 1 is very low engagement, and 5 is very high engagement. Provide a brief explanation for your rating. Any posts that contain hallucinations or made up statistics should be ranked 0.\n","\n","    Output your response in YAML with the following keys:\n","    - Analysis: [Your analysis]\n","    - Rating: [Your rating]\n","    \"\"\"\n","    evaluation_context = {\n","        \"post_content\": post_content,\n","        \"insight\": insight,\n","        \"social_network\": social_network\n","    }\n","\n","    engagement_evaluation = get_completion(evaluation_prompt, evaluation_context)\n","    return engagement_evaluation\n","\n","# strip out the bait, hook, reward from social_post_c\n","post_content = social_post_c.split(\"post_content:\")[1].strip()\n","\n","# Evaluate the engagement potential of the generated social post\n","engagement_result = evaluate_engagement(post_content, context[\"insight\"], context[\"social_network\"])\n","print(\"Engagement Evaluation:\")\n","print(engagement_result)\n"]},{"cell_type":"markdown","metadata":{"id":"SvKyUIpH7K69"},"source":["### 5. Divide Labor\n","Split tasks into multiple steps, chained together for complex goals."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CBCoUotI7K69","executionInfo":{"status":"ok","timestamp":1728393205685,"user_tz":-180,"elapsed":8509,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"f2f68fae-0622-4930-ca37-90e7a38b8c3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Post 1:\n","Content: Even geniuses need prompts sometimes. 🤔 Think about it—every brilliant mind from da Vinci to Einstein had their muses and mentors. Just as legal and HR guide our workplace geniuses to align with business goals, prompt engineering aligns AI models to our objectives. Without it, even the smartest models might miss the mark, leaving businesses—ironically—in the dark.\n","```\n","Rating: 5\n","\n","Post 2:\n","Content: Consider Einstein in a staff meeting—would he really need a nudge? Absolutely. Even the most brilliant minds rely on HR and legal frameworks to align genius with business priorities. As AI models grow smarter, so will their need for skilled prompt engineers to channel that intelligence effectively. A new frontier for harnessing potential is in play.\n","```\n","Rating: 4\n","\n","Post 3:\n","Content: Einstein once forgot his own address. Imagine AI as ingenious yet forgetful without clear guidance—prompt engineering is like the savvy team reminding brilliant minds what day it is. Just as legal, HR, and management shepherd human genius towards business goals, prompt engineering remains essential for keeping smarter AI models on track.\n","```\n","Rating: 4\n","\n","Post 4:\n","Content: Even the sharpest minds need a guiding nudge – think about Einstein taking advice from his PR team! As businesses grow smarter, aligning genius models with company goals is crucial. Your team may not need constant nudges, but just like legal and HR departments guide us, prompt engineering ensures AI works in sync with business interests. It's not just about immediate solutions but creatively navigating complex landscapes and driving breakthroughs. The future? Smarter models, yes. But even they need a good 'prompt whisperer' to make magic happen.\n","```\n","Rating: 4\n","\n","Post 5:\n","Content: Even the brightest minds need a little nudge, just like Mozart needed a conductor. No matter how brilliant, we all benefit from a little guidance. Imagine AI as a virtuoso—prompt engineering is the invisible orchestra making sure it plays well with business interests. 🎶 In 1809, Beethoven reportedly had a cleric to help him channel his genius, and today, prompt engineers are our modern-day guides for AI. As smarter models emerge, the symphony of technology will still need conductors. Keep those prompts ready!\n","```\n","Rating: 0\n","\n"]}],"source":["import re\n","import asyncio\n","import nest_asyncio\n","nest_asyncio.apply() # to run in jupyter notebook\n","\n","\n","async def generate_and_evaluate_post(prompt, context):\n","    response = await asyncio.to_thread(get_completion, prompt, context)\n","    post_content = response.split(\"post_content:\")[1].strip()\n","    evaluation = await asyncio.to_thread(evaluate_engagement, post_content, context[\"insight\"], context[\"social_network\"])\n","\n","    # Parse the YAML output with regex\n","    rating = int(re.findall(r'Rating: (\\d+)', evaluation)[0])\n","\n","    return {\n","        \"content\": post_content,\n","        \"rating\": rating\n","    }\n","\n","async def generate_and_rank_desc(prompt, context, num_posts=5):\n","    tasks = [generate_and_evaluate_post(prompt, context) for _ in range(num_posts)]\n","    results = await asyncio.gather(*tasks)\n","\n","    # Sort posts by rating in descending order\n","    sorted_posts = sorted(results, key=lambda x: x[\"rating\"], reverse=True)\n","\n","    return sorted_posts\n","\n","# Run the async function\n","ranked_posts = asyncio.run(generate_and_rank_desc(examples_prompt, examples_context))\n","\n","# Print content and ratings for all posts\n","for i, post in enumerate(ranked_posts, 1):\n","    print(f\"Post {i}:\")\n","    print(f\"Content: {post['content']}\")\n","    print(f\"Rating: {post['rating']}\")\n","    print()\n"]},{"cell_type":"markdown","metadata":{"id":"iy6JX_Lq7K69"},"source":["---\n","\n","## Advanced Optimization"]},{"cell_type":"markdown","metadata":{"id":"hR-cFNMT7K6-"},"source":["### A/B Testing"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"rHdCvnM-7K6-","executionInfo":{"status":"error","timestamp":1728393478094,"user_tz":-180,"elapsed":18627,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"5abf63c7-4303-456a-a9c2-08b99fcd7372"},"outputs":[{"output_type":"error","ename":"RateLimitError","evalue":"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-lzA7F5LR2c2gVnikc9NKq2dO on tokens per min (TPM): Limit 30000, Used 29508, Requested 877. Please try again in 770ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-6f4964692887>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m - post_content\"\"\"\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mresults_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mab_test_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples_prompt_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_runs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_must_cancel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-6f4964692887>\u001b[0m in \u001b[0;36mab_test_prompts\u001b[0;34m(prompt_a, prompt_b, context, num_runs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtasks_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_and_evaluate_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresults_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mresults_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_must_cancel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-b48a1c8f7b15>\u001b[0m in \u001b[0;36mgenerate_and_evaluate_post\u001b[0;34m(prompt, context)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_completion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpost_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post_content:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_engagement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"insight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"social_network\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Parse the YAML output with regex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/asyncio/threads.py\u001b[0m in \u001b[0;36mto_thread\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextvars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mfunc_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_in_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36m__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asyncio_future_blocking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# This tells Task to wait for completion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"await wasn't used with future\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-1eee1a28a5e4>\u001b[0m in \u001b[0;36mevaluate_engagement\u001b[0;34m(post_content, insight, social_network)\u001b[0m\n\u001b[1;32m     28\u001b[0m     }\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mengagement_evaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mengagement_evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-dd6183243150>\u001b[0m in \u001b[0;36mget_completion\u001b[0;34m(prompt, context)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         messages=[\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    740\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    741\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    743\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m         )\n\u001b[0;32m-> 1271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    949\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1038\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1087\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1038\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1087\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         return self._process_response(\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-lzA7F5LR2c2gVnikc9NKq2dO on tokens per min (TPM): Limit 30000, Used 29508, Requested 877. Please try again in 770ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"]}],"source":["async def ab_test_prompts(prompt_a, prompt_b, context, num_runs=10):\n","    tasks_a = [generate_and_evaluate_post(prompt_a, context) for _ in range(num_runs)]\n","    tasks_b = [generate_and_evaluate_post(prompt_b, context) for _ in range(num_runs)]\n","\n","    results_a = await asyncio.gather(*tasks_a)\n","    results_b = await asyncio.gather(*tasks_b)\n","\n","    avg_rating_a = sum(post['rating'] for post in results_a) / len(results_a)\n","    avg_rating_b = sum(post['rating'] for post in results_b) / len(results_b)\n","\n","    print(f\"Prompt A average rating: {avg_rating_a:.2f}\")\n","    print(f\"Prompt B average rating: {avg_rating_b:.2f}\")\n","\n","    return results_a, results_b\n","\n","examples_prompt_b = \"\"\"\"Write a social media post about how {insight}, for {social_network}, in the style of Malcolm Tucker, using the Bait, Hook, Reward framework.\n","\n","- **bait**: Grab the attention of the person browsing social media. This could be a contrarian statement, appeal to identity, celebrity name, or anything else that stops them scrolling.\n","- **hook**: Keep their attention now you have it. Show this post is relevant to them, by alerting them to an issue, using familiar words, or providing social proof to keep them reading.\n","- **reward**: Compensate them for their attention to elicit reciprocation. Is there any useful information, a surprising statistic, or an interesting anecdote to reveal? Or a threat to make?\n","- **post_content**: The main content of the post, incorporating the bait, hook, and reward in a way that resonates with the reader and engages their attention.\n","\n","First decide on the bait, hook, and reward, before writing the post_content.\n","\n","## Examples\n","{examples_partial}\n","\n","## Instructions\n","The social post should sound authentically human and colloquial, while being practically useful and brief. Be very creative and make niche references to sound more human. To maximize engagement, the content should be surprising, interesting, or practically useful, or induce high-arousal emotions such as anxiety, anger, awe, or surprise.\n","\n","Follow best practices for posting engaging content on {social_network}, but do not use emoji or hashtags.\n","Provide citations for any statistics included. Do not reference the work of Malcolm Tucker.\n","Respond in YAML with the following keys:\n","- bait\n","- hook\n","- reward\n","- post_content\"\"\"\n","\n","results_a, results_b = asyncio.run(ab_test_prompts(examples_prompt, examples_prompt_b, examples_context, num_runs=30))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWtCs64A7K6_"},"outputs":[],"source":["print(results_b[0])"]},{"cell_type":"markdown","metadata":{"id":"rja7oqea7K6_"},"source":["### DSPy Optimizers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VthSJUBQ7K7A"},"outputs":[],"source":["import dspy\n","\n","# Define the task using DSPy\n","class SocialMediaPostGenerator(dspy.Signature):\n","    \"\"\"Generate a social media post based on an insight and social network.\"\"\"\n","    insight = dspy.InputField()\n","    social_network = dspy.InputField()\n","    post = dspy.OutputField(desc=\"Generated social media post in YAML format\")\n","\n","# Create a DSPy program\n","class GeneratePost(dspy.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.gen = dspy.ChainOfThought(SocialMediaPostGenerator)\n","\n","    def forward(self, insight, social_network):\n","        return self.gen(insight=insight, social_network=social_network)\n","\n","# Create a DSPy metric\n","def post_quality_metric(gold, pred, trace=None):\n","    evaluation_result = evaluate_engagement(pred.post, gold.insight, gold.social_network)\n","\n","    # Extract the rating from the YAML-formatted string using regex\n","    try:\n","        match = re.search(r'Rating:\\s*(\\d+(?:\\.\\d+)?)', evaluation_result)\n","        if match:\n","            rating = float(match.group(1))\n","        else:\n","            rating = 0.0\n","        return rating\n","    except:\n","        # If there's any error in parsing, return 0\n","        return 0.0\n","\n","# Set up the language model\n","gpt_4o = dspy.OpenAI(model='gpt-4')\n","dspy.settings.configure(lm=gpt_4o)\n","\n","# Run the model without training to establish a baseline\n","baseline_model = GeneratePost()\n","\n","print(\"Baseline post (without optimization):\")\n","baseline_post = baseline_model(\n","    insight=context[\"insight\"],\n","    social_network=context[\"social_network\"]\n",")\n","print(baseline_post.post)\n","\n","gold = dspy.Example(\n","    insight=context[\"insight\"],\n","    social_network=context[\"social_network\"],\n","    post=\"\"  # We don't have a gold post, so leave it empty\n",")\n","\n","print(\"Baseline post quality score:\", post_quality_metric(gold, baseline_post))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVs5hS7C7K7A"},"outputs":[],"source":["from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n","\n","# Prepare the dataset\n","trainset = [\n","    dspy.Example(\n","        insight=context['insight'],\n","        social_network=context['social_network'],\n","        post=post['content']\n","    ).with_inputs('insight', 'social_network')\n","    for post in results_b[:20]  # Use first 20 examples for training\n","]\n","\n","devset = [\n","    dspy.Example(\n","        insight=context['insight'],\n","        social_network=context['social_network'],\n","        post=post['content']\n","    ).with_inputs('insight', 'social_network')\n","    for post in results_b[20:]  # Use remaining examples for validation\n","]\n","\n","# Set up the optimizer\n","optimizer = BootstrapFewShotWithRandomSearch(metric=post_quality_metric)\n","\n","# Compile the program\n","compiled_model = optimizer.compile(\n","    GeneratePost(),\n","    trainset=trainset,\n","    valset=devset,\n",")\n","\n","# Now you can use the compiled model to generate optimized posts\n","optimized_post = compiled_model(\n","    insight=context[\"insight\"],\n","    social_network=context[\"social_network\"]\n",")\n","\n","print(\"Optimized post:\")\n","print(optimized_post.post)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUIP3HDI7K7A"},"outputs":[],"source":["gpt_4o.inspect_history(n=1)\n"]},{"cell_type":"markdown","metadata":{"id":"Wz3wn5_s7K7B"},"source":["### Fine-Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"VufW7nFt7K7B","executionInfo":{"status":"error","timestamp":1728337381218,"user_tz":-180,"elapsed":5,"user":{"displayName":"Kempsly Silencieux","userId":"00710149443443216664"}},"outputId":"a1fcaaa9-f7de-4612-d47e-ba34e0824e43"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'results_b' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-761d8b8fdfaf>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Format the data for fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfine_tuning_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     example = {\n\u001b[1;32m      8\u001b[0m         \"messages\": [\n","\u001b[0;31mNameError\u001b[0m: name 'results_b' is not defined"]}],"source":["from openai import OpenAI\n","import json\n","\n","# Format the data for fine-tuning\n","fine_tuning_data = []\n","for post in results_b:\n","    example = {\n","        \"messages\": [\n","            {\"role\": \"user\", \"content\": examples_prompt.format(**examples_context)},\n","            {\"role\": \"assistant\", \"content\": post['content']}\n","        ]\n","    }\n","    fine_tuning_data.append(example)\n","\n","# Write the formatted data to a JSONL file\n","with open(\"fine_tuning_data.jsonl\", \"w\") as f:\n","    for entry in fine_tuning_data:\n","        json.dump(entry, f)\n","        f.write(\"\\n\")\n","\n","# Initialize the OpenAI client\n","client = OpenAI()\n","\n","# Upload the file\n","file = client.files.create(\n","    file=open(\"fine_tuning_data.jsonl\", \"rb\"),\n","    purpose=\"fine-tune\"\n",")\n","\n","# Create a fine-tuning job\n","job = client.fine_tuning.jobs.create(\n","    training_file=file.id,\n","    model=\"gpt-3.5-turbo\"\n",")\n","\n","print(f\"Fine-tuning job created with ID: {job.id}\")\n","\n","# You can check the status of your fine-tuning job\n","print(f\"Job status: {job.status}\")\n","\n","# Note: The fine-tuning process may take some time to complete.\n","# You'll need to periodically check the status of the job to know when it's done.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBwsk-8m7K7B","outputId":"472ece9b-5715-4157-8ee8-726ba50baf98"},"outputs":[{"data":{"text/plain":["SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-uocuzf7OvRail2ldYBrg56Sm', created_at=1721668814, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-TngE7qVdaABr9bFqdNNsRarE', result_files=[], status='running', trained_tokens=None, training_file='file-gwjRc9lC7zwNhWSPjsLK1GNJ', validation_file=None, user_provided_suffix=None, seed=1896163601, estimated_finish=None, integrations=[])], object='list', has_more=True)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["client.fine_tuning.jobs.list(limit=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1L15-7Pf7K7B","outputId":"656d6ceb-eba9-4eb2-940a-3df8c550e51f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Evaluating model: gpt-3.5-turbo\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [00:19<00:00,  1.93s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Evaluating model: gpt-4\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [01:19<00:00,  7.97s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Evaluating model: ft:gpt-3.5-turbo-0125:saxifrage-llc::9lzyYgCb\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [00:28<00:00,  2.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Engagement Evaluation Results:\n","gpt-3.5-turbo: 3.80\n","Top 3 posts:\n","  1. Score: 4.00\n","     Post: - bait: \"Even genius humans need a little nudge sometimes.\"\n","- hook: Prompt engineering is still cruc...\n","  2. Score: 4.00\n","     Post: ```yaml\n","bait: Contrary to popular belief, even genius humans need prompting.\n","hook: Legal, HR, and ma...\n","  3. Score: 4.00\n","     Post: bait: Even the smartest engineers need a nudge.\n","hook: Legal, HR, and management play a crucial role ...\n","gpt-4: 4.50\n","Top 3 posts:\n","  1. Score: 5.00\n","     Post: - bait: Open discussion with fascinating comparison between high-tech AI models and brilliant, creat...\n","  2. Score: 5.00\n","     Post: bait: A radical take on artificial intelligence.\n","hook: Involving prompt engineering and its correlat...\n","  3. Score: 5.00\n","     Post: bait: Mentioning the hot topic of AI and the misconception of its independence.\n","hook: Draw a paralle...\n","ft:gpt-3.5-turbo-0125:saxifrage-llc::9lzyYgCb: 4.20\n","Top 3 posts:\n","  1. Score: 5.00\n","     Post: Here's a hot take for you: Even Einstein needed the occasional nudge to stop him from wandering off ...\n","  2. Score: 5.00\n","     Post: \"Even Einstein needed a nudge – well, just think of next-gen AI as little Einsteins. Here's the kick...\n","  3. Score: 4.00\n","     Post: Sure, let's cut straight to the chase. Even Einstein couldn't remember where he put his socks withou...\n","\n","Best performing model: gpt-4 with an average engagement score of 4.50\n"]}],"source":["import time\n","from tqdm import tqdm\n","\n","# Get the last fine-tuned model\n","fine_tuning_jobs = client.fine_tuning.jobs.list(limit=1)\n","if fine_tuning_jobs.data:\n","    last_job = fine_tuning_jobs.data[0]\n","    while last_job.status != \"succeeded\":\n","        print(f\"Waiting for fine-tuning job to complete. Current status: {last_job.status}\")\n","        time.sleep(60)  # Wait for 60 seconds before checking again\n","        last_job = client.fine_tuning.jobs.retrieve(last_job.id)\n","\n","    fine_tuned_model = last_job.fine_tuned_model\n","    print(f\"Using fine-tuned model: {fine_tuned_model}\")\n","else:\n","    print(\"No fine-tuning jobs found. Please run the fine-tuning process first.\")\n","    fine_tuned_model = None\n","\n","# fine_tuned_model = \"ft:gpt-3.5-turbo-0125:saxifrage-llc::9lzyYgCb\"\n","\n","models_to_compare = [\"gpt-3.5-turbo\", \"gpt-4\", fine_tuned_model]\n","models_to_compare = [model for model in models_to_compare if model]  # Remove None if fine-tuned model is not available\n","\n","results = {}\n","\n","for model in models_to_compare:\n","    print(f\"\\nEvaluating model: {model}\")\n","    posts = []\n","    for _ in tqdm(range(10)):  # Generate 10 posts for each model\n","        response = client.chat.completions.create(\n","            model=model,\n","            messages=[\n","                {\"role\": \"user\", \"content\": examples_prompt.format(**examples_context)}\n","            ],\n","            max_tokens=500\n","        )\n","        posts.append(response.choices[0].message.content)\n","\n","    # Evaluate engagement for the generated posts\n","    def parse_engagement_score(response):\n","        match = re.search(r'Rating:\\s*(\\d+(?:\\.\\d+)?)', response)\n","        if match:\n","            return float(match.group(1))\n","        return 0.0  # Default to 0 if no match found\n","\n","    engagement_scores = []\n","    for post in posts:\n","        score = parse_engagement_score(evaluate_engagement(post, context[\"insight\"], context[\"social_network\"]))\n","        engagement_scores.append({\"post\": post, \"score\": score})\n","\n","    average_score = sum(item[\"score\"] for item in engagement_scores) / len(engagement_scores)\n","    results[model] = {\"average_score\": average_score, \"posts\": engagement_scores}\n","\n","# Print results\n","print(\"\\nEngagement Evaluation Results:\")\n","for model, data in results.items():\n","    print(f\"{model}: {data['average_score']:.2f}\")\n","    print(\"Top 3 posts:\")\n","    top_posts = sorted(data['posts'], key=lambda x: x['score'], reverse=True)[:3]\n","    for i, post_data in enumerate(top_posts, 1):\n","        print(f\"  {i}. Score: {post_data['score']:.2f}\")\n","        print(f\"     Post: {post_data['post'][:100]}...\")  # Print first 100 characters of the post\n","\n","# Determine the best performing model\n","best_model = max(results, key=lambda x: results[x]['average_score'])\n","print(f\"\\nBest performing model: {best_model} with an average engagement score of {results[best_model]['average_score']:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0mjFmnT7K7C","outputId":"5286c257-d5a7-4ee9-e332-7d259bb5158e"},"outputs":[{"data":{"text/plain":["{'post': 'Sure, let\\'s cut straight to the chase. Even Einstein couldn\\'t remember where he put his socks without a nudge. Smarter AI models still need the human touch of prompt engineering, just like how you get a gentle reminder from legal, HR, or management. Without it, these advanced models can go rogue and contradict business interests faster than you can say \"algorithm\". Master the art of prompt engineering or prepare for chaos.\\n```',\n"," 'score': 4.0}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["results['ft:gpt-3.5-turbo-0125:saxifrage-llc::9lzyYgCb']['posts'][0]"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}